{"cells":[{"cell_type":"markdown","metadata":{},"source":["Observação: necessário baixar base de dados Sentiment140 e colocá-la na pasta input:\n","https://www.kaggle.com/datasets/kazanova/sentiment140"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T23:17:19.341416Z","iopub.status.busy":"2023-09-25T23:17:19.341044Z","iopub.status.idle":"2023-09-25T23:46:43.582917Z","shell.execute_reply":"2023-09-25T23:46:43.581916Z","shell.execute_reply.started":"2023-09-25T23:17:19.341385Z"},"trusted":true},"outputs":[],"source":["import re\n","import emoji\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import seaborn as sns\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","import spacy\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import train_test_split\n","from gensim.models import Word2Vec\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","DATASET_COLUMNS = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n","DATASET_ENCODING = \"ISO-8859-1\"\n","dataset = pd.read_csv('./input/training.1600000.processed.noemoticon.csv',\n","                      encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n","dataset.head()\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","stop_words = nlp.Defaults.stop_words\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","dataset = dataset[['sentiment', 'text']]\n","\n","dataset['sentiment'] = dataset['sentiment'].replace(0, 1)\n","\n","dataset['sentiment'] = dataset['sentiment'].replace(4, 0)\n","\n","contractions = pd.read_csv(\n","    './input/contractions.csv', index_col='Contraction')\n","contractions.index = contractions.index.str.lower()\n","contractions.Meaning = contractions.Meaning.str.lower()\n","contractions_dict = contractions.to_dict()['Meaning']\n","\n","urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n","userPattern = '@[^\\s]+'\n","hashtagPattern = '#[^\\s]+'\n","alphaPattern = \"[^a-z0-9<>]\"\n","sequencePattern = r\"(.)\\1\\1+\"\n","seqReplacePattern = r\"\\1\\1\"\n","\n","\n","def preprocess_apply(tweet):\n","\n","    # Tansform all the tweets to LowerCase\n","    tweet = tweet.lower()\n","\n","    # Remove Emojis\n","    emoji_pattern = re.compile(\"[\"\n","                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                               u\"\\U00002702-\\U000027B0\"\n","                               u\"\\U000024C2-\\U0001F251\"\n","                               \"]+\", flags=re.UNICODE)\n","    tweet = emoji_pattern.sub(r'', tweet)\n","\n","    # Strip all URLs\n","    tweet = re.sub(urlPattern, '', tweet)\n","\n","    # Remove mentions\n","    tweet = re.sub(userPattern, '', tweet)\n","\n","    # Remove Hashtags from the end of the sentence\n","    tweet = re.sub(r'(\\s+#[\\w-]+)+\\s*$', '', tweet).strip()\n","\n","    # Remove the # symbol from hashtags in the middle of the sentence\n","    tweet = re.sub(r'#([\\w-]+)', r'\\1', tweet).strip()\n","\n","    # Replace 3 or more consecutive letters by 2 letter.\n","    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n","\n","    # Remove Multiple Spaces\n","    tweet = re.sub(r\"\\s\\s+\", \" \", tweet)\n","\n","    # Remove Numbers\n","    tweet = re.sub(r'\\d+', '', tweet)\n","\n","    for contraction, replacement in contractions_dict.items():\n","        tweet = tweet.replace(contraction, replacement)\n","\n","    # Remove non-alphanumeric and symbols\n","    tweet = re.sub(alphaPattern, ' ', tweet)\n","\n","    # Adding space on either side of '/' to seperate words (After replacing URLS).\n","    tweet = re.sub(r'/', ' / ', tweet)\n","\n","    # Lemmatize the text\n","    words = word_tokenize(tweet)\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n","    tweet = ' '.join(lemmatized_words)\n","\n","    return tweet\n","\n","\n","tqdm.pandas()\n","dataset['processed_text'] = dataset.text.progress_apply(preprocess_apply)\n","print(\"[!] Cleaning Done!\")\n","\n","X_data, y_data = np.array(\n","    dataset['processed_text']), np.array(dataset['sentiment'])\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_data, y_data, test_size=0.3, random_state=0)\n","print('[!] Data Split done.')\n","\n","Embedding_dimensions = 100\n","Word2vec_train_data = list(map(lambda x: x.split(), X_train))\n","\n","word2vec_model = Word2Vec(\n","    Word2vec_train_data, vector_size=Embedding_dimensions, workers=8, min_count=5)\n","print(\"[*] Vocabulary Length:\", len(word2vec_model.wv.key_to_index))\n","\n","input_length = 60\n","vocab_length = 60000\n","\n","tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n","tokenizer.fit_on_texts(X_data)\n","tokenizer.num_words = vocab_length\n","print(\"[*] Tokenizer vocab length:\", vocab_length)\n","\n","X_train = pad_sequences(\n","    tokenizer.texts_to_sequences(X_train), maxlen=input_length)\n","X_test = pad_sequences(\n","    tokenizer.texts_to_sequences(X_test), maxlen=input_length)\n","\n","print(\"[!] X_train.shape:\", X_train.shape)\n","print(\"[!] X_test.shape :\", X_test.shape)\n","\n","embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n","\n","for word, token in tokenizer.word_index.items():\n","    if word2vec_model.wv.__contains__(word):\n","        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n","\n","print(\"[*] Embedding Matrix Shape:\", embedding_matrix.shape)\n","\n","\n","def getModel():\n","    embedding_layer = Embedding(input_dim=vocab_length,\n","                                output_dim=Embedding_dimensions,\n","                                weights=[embedding_matrix],\n","                                input_length=input_length,\n","                                trainable=False)\n","\n","    model = Sequential([\n","        embedding_layer,\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Conv1D(100, 5, activation='relu'),\n","        GlobalMaxPool1D(),\n","        Dense(16, activation='relu'),\n","        Dense(1, activation='sigmoid'),\n","    ],\n","        name=\"Sentiment_Model\")\n","    return model\n","\n","\n","training_model = getModel()\n","training_model.summary()\n","\n","callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0), EarlyStopping(\n","    monitor='val_accuracy', min_delta=1e-4, patience=5)]\n","\n","training_model.compile(loss='binary_crossentropy',\n","                       optimizer='adam', metrics=['accuracy'])\n","\n","history = training_model.fit(\n","    X_train, y_train,\n","    batch_size=1024,\n","    epochs=12,\n","    validation_split=0.1,\n","    callbacks=callbacks,\n","    verbose=1,\n",")\n","\n","word2vec_model.wv.save('Word2Vec-twitter-100')\n","word2vec_model.wv.save_word2vec_format('Word2Vec-twitter-100-trainable')\n","\n","with open('Tokenizer.pickle', 'wb') as file:\n","    pickle.dump(tokenizer, file)\n","\n","training_model.save('Sentiment-BiLSTM')\n","training_model.save_weights(\"Model Weights/weights\")\n","\n","acc,  val_acc = history.history['accuracy'], history.history['val_accuracy']\n","loss, val_loss = history.history['loss'], history.history['val_loss']\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'b', label='Training acc')\n","plt.plot(epochs, val_acc, 'r', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'b', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
